{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZVirBC7PDc4"
      },
      "source": [
        "# Assignment 2A\n",
        "\n",
        "**Name**: RAHUL NARAYAN                              </br>\n",
        "**Roll No.**: 210806"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_skarwBuPDc5",
        "outputId": "2e97b41b-8566-4600-f9ec-d848df348c75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/953.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/953.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m686.1/953.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m952.3/953.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.10.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium\n",
        "# import\n",
        "import gymnasium as gym\n",
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Oc8foh2vPDc5"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Represents a Stochastic Maze problem Gym Environment which provides a Fully observable\n",
        "MDP\n",
        "'''\n",
        "class StochasticMazeEnv(gym.Env):\n",
        "    '''\n",
        "    StochasticMazeEnv represents the Gym Environment for the Stochastic Maze environment\n",
        "    States : [0,1,2,3,4,5,6,7,8,9,10,11]\n",
        "    Actions : [\"Left\":0, \"Up\":1, \"Right\":2, \"Down\":3]\n",
        "    '''\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self,initial_state=0,no_states=12,no_actions=4):\n",
        "        '''\n",
        "        Constructor for the StochasticMazeEnv class\n",
        "\n",
        "        Args:\n",
        "            initial_state : starting state of the agent\n",
        "            no_states : The no. of possible states which is 12\n",
        "            no_actions : The no. of possible actions which is 4\n",
        "\n",
        "        '''\n",
        "        self.initial_state = initial_state\n",
        "        self.state = self.initial_state\n",
        "        self.nA = no_actions\n",
        "        self.nS = no_states\n",
        "        self.actions_dict = {\"L\":0, \"U\":1, \"R\":2, \"D\":3}\n",
        "        self.prob_dynamics = {\n",
        "            0: {\n",
        "                0: [(0.8, 0, -0.01, False), (0.1, 0, -0.01, False), (0.1, 4, -0.01, False)],\n",
        "                1: [(0.8, 0, -0.01, False), (0.1, 0, -0.01, False), (0.1, 1, -0.01, False)],\n",
        "                2: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 4, -0.01, False)],\n",
        "                3: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 1, -0.01, False)],\n",
        "            },\n",
        "            1: {\n",
        "                0: [(0.8, 0, -0.01, False), (0.1, 1, -0.01, False), (0.1, 1, -0.01, False)],\n",
        "                1: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 2, -0.01, False)],\n",
        "                2: [(0.8, 2, -0.01, False), (0.1, 1, -0.01, False), (0.1, 1, -0.01, False)],\n",
        "                3: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 2, -0.01, False)],\n",
        "            },\n",
        "            2: {\n",
        "                0: [(0.8, 1, -0.01, False), (0.1, 2, -0.01, False), (0.1, 6, -0.01, False)],\n",
        "                1: [(0.8, 2, -0.01, False), (0.1, 1, -0.01, False), (0.1, 3, +1, True)],\n",
        "                2: [(0.8, 3, +1, True), (0.1, 2, -0.01, False), (0.1, 6, -0.01, False)],\n",
        "                3: [(0.8, 6, -0.01, False), (0.1, 1, -0.01, False), (0.1, 3, +1, True)],\n",
        "            },\n",
        "            3: {\n",
        "                0: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
        "                1: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
        "                2: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
        "                3: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
        "            },\n",
        "            4: {\n",
        "                0: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 8, -0.01, False)],\n",
        "                1: [(0.8, 0, -0.01, False), (0.1, 4, -0.01, False), (0.1, 4, -0.01, False)],\n",
        "                2: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 8, -0.01, False)],\n",
        "                3: [(0.8, 8, -0.01, False), (0.1, 4, -0.01, False), (0.1, 4, -0.01, False)],\n",
        "            },\n",
        "            5: {\n",
        "                0: [(0.8, 5, 0, True), (0.1, 5, 0, True), (0.1, 5, 0, True)],\n",
        "                1: [(0.8, 5, 0, True), (0.1, 5, 0, True), (0.1, 5, 0, True)],\n",
        "                2: [(0.8, 5, 0, True), (0.1, 5, 0, True), (0.1, 5, 0, True)],\n",
        "                3: [(0.8, 5, 0, True), (0.1, 5, 0, True), (0.1, 5, 0, True)],\n",
        "            },\n",
        "            6: {\n",
        "                0: [(0.8, 6, -0.01, False), (0.1, 2, -0.01, False), (0.1, 10, -0.01, False)],\n",
        "                1: [(0.8, 2, -0.01, False), (0.1, 6, -0.01, False), (0.1, 7, -1, True)],\n",
        "                2: [(0.8, 7, -1, True), (0.1, 2, -0.01, False), (0.1, 10, -0.01, False)],\n",
        "                3: [(0.8, 10, -0.01, False), (0.1, 6, -0.01, False), (0.1, 7, -1, True)],\n",
        "            },\n",
        "            7: {\n",
        "                0: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
        "                1: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
        "                2: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
        "                3: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
        "            },\n",
        "            8: {\n",
        "                0: [(0.8, 8, -0.01, False), (0.1, 8, -0.01, False), (0.1, 4, -0.01, False)],\n",
        "                1: [(0.8, 4, -0.01, False), (0.1, 8, -0.01, False), (0.1, 9, -0.01, False)],\n",
        "                2: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 4, -0.01, False)],\n",
        "                3: [(0.8, 8, -0.01, False), (0.1, 8, -0.01, False), (0.1, 9, -0.01, False)],\n",
        "            },\n",
        "            9: {\n",
        "                0: [(0.8, 8, -0.01, False), (0.1, 9, -0.01, False), (0.1, 9, -0.01, False)],\n",
        "                1: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 10, -0.01, False)],\n",
        "                2: [(0.8, 10, -0.01, False), (0.1, 9, -0.01, False), (0.1, 9, -0.01, False)],\n",
        "                3: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 10, -0.01, False)]\n",
        "            },\n",
        "            10: {\n",
        "                0: [(0.8, 9, -0.01, False), (0.1, 6, -0.01, False), (0.1, 10, -0.01, False)],\n",
        "                1: [(0.8, 6, -0.01, False), (0.1, 9, -0.01, False), (0.1, 11, -0.01, False)],\n",
        "                2: [(0.8, 11, -0.01, False), (0.1, 6, -0.01, False), (0.1, 10, -0.01, False)],\n",
        "                3: [(0.8, 10, -0.01, False), (0.1, 9, -0.01, False), (0.1, 11, -0.01, False)]\n",
        "            },\n",
        "            11: {\n",
        "                0: [(0.8, 10, -0.01, False), (0.1, 7, -1, True), (0.1, 11, -0.01, False)],\n",
        "                1: [(0.8, 7, -1, True), (0.1, 10, -0.01, False), (0.1, 11, -0.01, False)],\n",
        "                2: [(0.8, 11, -0.01, False), (0.1, 7, -1, True), (0.1, 11, -0.01, False)],\n",
        "                3: [(0.8, 11, -0.01, False), (0.1, 11, -0.01, False), (0.1, 10, -0.01, False)]\n",
        "            }\n",
        "        }\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        '''\n",
        "        Resets the environment\n",
        "        Returns:\n",
        "            observations containing player's current state\n",
        "        '''\n",
        "        self.state = self.initial_state\n",
        "        return self.get_obs()\n",
        "\n",
        "    def get_obs(self):\n",
        "        '''\n",
        "        Returns the player's state as the observation of the environment\n",
        "        '''\n",
        "        return (self.state)\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        '''\n",
        "        Renders the environment\n",
        "        '''\n",
        "        print(\"Current state: {}\".format(self.state))\n",
        "\n",
        "    def sample_action(self):\n",
        "        '''\n",
        "        Samples and returns a random action from the action space\n",
        "        '''\n",
        "        return random.randint(0, self.nA)\n",
        "    def P(self):\n",
        "        '''\n",
        "        Defines and returns the probabilty transition matrix which is in the form of a nested dictionary\n",
        "        '''\n",
        "        self.prob_dynamics = {\n",
        "            0: {\n",
        "                0: [(0.8, 0, -0.01, False), (0.1, 0, -0.01, False), (0.1, 4, -0.01, False)],\n",
        "                1: [(0.8, 0, -0.01, False), (0.1, 0, -0.01, False), (0.1, 1, -0.01, False)],\n",
        "                2: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 4, -0.01, False)],\n",
        "                3: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 1, -0.01, False)],\n",
        "            },\n",
        "            1: {\n",
        "                0: [(0.8, 0, -0.01, False), (0.1, 1, -0.01, False), (0.1, 1, -0.01, False)],\n",
        "                1: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 2, -0.01, False)],\n",
        "                2: [(0.8, 2, -0.01, False), (0.1, 1, -0.01, False), (0.1, 1, -0.01, False)],\n",
        "                3: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 2, -0.01, False)],\n",
        "            },\n",
        "            2: {\n",
        "                0: [(0.8, 1, -0.01, False), (0.1, 2, -0.01, False), (0.1, 6, -0.01, False)],\n",
        "                1: [(0.8, 2, -0.01, False), (0.1, 1, -0.01, False), (0.1, 3, +1, True)],\n",
        "                2: [(0.8, 3, +1, True), (0.1, 2, -0.01, False), (0.1, 6, -0.01, False)],\n",
        "                3: [(0.8, 6, -0.01, False), (0.1, 1, -0.01, False), (0.1, 3, +1, True)],\n",
        "            },\n",
        "            3: {\n",
        "                0: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
        "                1: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
        "                2: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
        "                3: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
        "            },\n",
        "            4: {\n",
        "                0: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 8, -0.01, False)],\n",
        "                1: [(0.8, 0, -0.01, False), (0.1, 4, -0.01, False), (0.1, 4, -0.01, False)],\n",
        "                2: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 8, -0.01, False)],\n",
        "                3: [(0.8, 8, -0.01, False), (0.1, 4, -0.01, False), (0.1, 4, -0.01, False)],\n",
        "            },\n",
        "            5: {\n",
        "                0: [(0.8, 5, 0, True), (0.1, 5, 0, True), (0.1, 5, 0, True)],\n",
        "                1: [(0.8, 5, 0, True), (0.1, 5, 0, True), (0.1, 5, 0, True)],\n",
        "                2: [(0.8, 5, 0, True), (0.1, 5, 0, True), (0.1, 5, 0, True)],\n",
        "                3: [(0.8, 5, 0, True), (0.1, 5, 0, True), (0.1, 5, 0, True)],\n",
        "            },\n",
        "\n",
        "            6: {\n",
        "                0: [(0.8, 6, -0.01, False), (0.1, 2, -0.01, False), (0.1, 10, -0.01, False)],\n",
        "                1: [(0.8, 2, -0.01, False), (0.1, 6, -0.01, False), (0.1, 7, -1, True)],\n",
        "                2: [(0.8, 7, -1, True), (0.1, 2, -0.01, False), (0.1, 10, -0.01, False)],\n",
        "                3: [(0.8, 10, -0.01, False), (0.1, 6, -0.01, False), (0.1, 7, -1, True)],\n",
        "            },\n",
        "            7: {\n",
        "                0: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
        "                1: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
        "                2: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
        "                3: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
        "            },\n",
        "            8: {\n",
        "                0: [(0.8, 8, -0.01, False), (0.1, 8, -0.01, False), (0.1, 4, -0.01, False)],\n",
        "                1: [(0.8, 4, -0.01, False), (0.1, 8, -0.01, False), (0.1, 9, -0.01, False)],\n",
        "                2: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 4, -0.01, False)],\n",
        "                3: [(0.8, 8, -0.01, False), (0.1, 8, -0.01, False), (0.1, 9, -0.01, False)],\n",
        "            },\n",
        "            9: {\n",
        "                0: [(0.8, 8, -0.01, False), (0.1, 9, -0.01, False), (0.1, 9, -0.01, False)],\n",
        "                1: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 10, -0.01, False)],\n",
        "                2: [(0.8, 10, -0.01, False), (0.1, 9, -0.01, False), (0.1, 9, -0.01, False)],\n",
        "                3: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 10, -0.01, False)]\n",
        "            },\n",
        "            10: {\n",
        "                0: [(0.8, 9, -0.01, False), (0.1, 6, -0.01, False), (0.1, 10, -0.01, False)],\n",
        "                1: [(0.8, 6, -0.01, False), (0.1, 9, -0.01, False), (0.1, 11, -0.01, False)],\n",
        "                2: [(0.8, 11, -0.01, False), (0.1, 6, -0.01, False), (0.1, 10, -0.01, False)],\n",
        "                3: [(0.8, 10, -0.01, False), (0.1, 9, -0.01, False), (0.1, 11, -0.01, False)]\n",
        "            },\n",
        "            11: {\n",
        "                0: [(0.8, 10, -0.01, False), (0.1, 7, -1, True), (0.1, 11, -0.01, False)],\n",
        "                1: [(0.8, 7, -1, True), (0.1, 10, -0.01, False), (0.1, 11, -0.01, False)],\n",
        "                2: [(0.8, 11, -0.01, False), (0.1, 7, -1, True), (0.1, 11, -0.01, False)],\n",
        "                3: [(0.8, 11, -0.01, False), (0.1, 11, -0.01, False), (0.1, 10, -0.01, False)]\n",
        "            }\n",
        "        }\n",
        "        return self.prob_dynamics\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "        '''\n",
        "        Performs the given action\n",
        "        Args:\n",
        "            action : action from the action_space to be taking in the environment\n",
        "        Returns:\n",
        "            observation - returns current state\n",
        "            reward - reward obtained after taking the given action\n",
        "            done - True if the episode is complete else False\n",
        "        '''\n",
        "        if action >= self.nA:\n",
        "            action = self.nA-1\n",
        "\n",
        "        index = np.random.choice(3,1,p=[0.8,0.1,0.1])[0]\n",
        "\n",
        "        dynamics_tuple = self.prob_dynamics[self.state][action][index]\n",
        "        self.state = dynamics_tuple[1]\n",
        "\n",
        "\n",
        "        return self.state, dynamics_tuple[2], dynamics_tuple[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3stckvozPDc7"
      },
      "outputs": [],
      "source": [
        "env = StochasticMazeEnv()\n",
        "env.reset()\n",
        "num_states = env.nS\n",
        "num_actions = env.nA\n",
        "l=env.prob_dynamics[1][2][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O22G37FzPDc7"
      },
      "source": [
        "## Test Cases for checking the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0IJVM5DPDc7"
      },
      "source": [
        "## Example: Random Policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnBk8hn_PDc7",
        "outputId": "00d0da35-3c74-4138-ed86-2caa6730befc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action\t State\t Reward\t is_Terminal\n",
            "   3 \t   4 \t -0.01 \t   False\n",
            "   3 \t   8 \t -0.01 \t   False\n",
            "   1 \t   4 \t -0.01 \t   False\n",
            "   1 \t   0 \t -0.01 \t   False\n",
            "   0 \t   0 \t -0.01 \t   False\n",
            "   1 \t   0 \t -0.01 \t   False\n",
            "   2 \t   1 \t -0.01 \t   False\n",
            "   2 \t   2 \t -0.01 \t   False\n",
            "   1 \t   2 \t -0.01 \t   False\n",
            "   1 \t   2 \t -0.01 \t   False\n",
            "   1 \t   1 \t -0.01 \t   False\n",
            "   2 \t   1 \t -0.01 \t   False\n",
            "   1 \t   1 \t -0.01 \t   False\n",
            "   0 \t   0 \t -0.01 \t   False\n",
            "   2 \t   1 \t -0.01 \t   False\n",
            "   3 \t   1 \t -0.01 \t   False\n",
            "   3 \t   1 \t -0.01 \t   False\n",
            "   2 \t   2 \t -0.01 \t   False\n",
            "   2 \t   3 \t 1 \t   True\n",
            "Total Number of steps to Reach Terminal: 19\n",
            "Final Reward: 0.82\n"
          ]
        }
      ],
      "source": [
        "is_Terminal = False\n",
        "env.reset()\n",
        "count = 0\n",
        "total_reward = 0\n",
        "\n",
        "print(\"Action\\t\" , \"State\\t\" , \"Reward\\t\" , \"is_Terminal\")\n",
        "\n",
        "while not is_Terminal:\n",
        "    count += 1\n",
        "\n",
        "    rand_action = np.random.choice(4,1)[0]  #0 -> LEFT, 1 -> UP, 2 -> RIGHT, 3 -> DOWN\n",
        "    state, reward, is_Terminal = env.step(rand_action)\n",
        "    total_reward += reward\n",
        "\n",
        "    print(\"  \", rand_action, \"\\t  \", state, \"\\t\", reward, \"\\t  \", is_Terminal)\n",
        "print(\"Total Number of steps to Reach Terminal:\", count)\n",
        "print(\"Final Reward:\", total_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WKTQpXuPDc7"
      },
      "source": [
        "### The random policy takes large number of steps to reach some terminal state which should be much higher than the number of the steps taken by a all 'Right' policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fU0I42ePPDc7"
      },
      "source": [
        "## Example: Right Policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bqmr_tLPDc7",
        "outputId": "d7660cc7-f326-4f91-c48a-5bd061232073"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action\t State\t Reward\t is_Terminal\n",
            "   2 \t   1 \t -0.01 \t   False\n",
            "   2 \t   2 \t -0.01 \t   False\n",
            "   2 \t   3 \t 1 \t   True\n",
            "Total Number of steps to Reach Terminal: 3\n",
            "Final Reward: 0.98\n"
          ]
        }
      ],
      "source": [
        "is_Terminal = False\n",
        "env.reset()\n",
        "count = 0\n",
        "total_reward = 0\n",
        "\n",
        "print(\"Action\\t\" , \"State\\t\" , \"Reward\\t\" , \"is_Terminal\")\n",
        "\n",
        "while not is_Terminal:\n",
        "    count += 1\n",
        "\n",
        "    right_action = 2  #0 -> LEFT, 1 -> UP, 2 -> RIGHT, 3 -> DOWN\n",
        "    state, reward, is_Terminal = env.step(right_action)\n",
        "\n",
        "    total_reward += reward\n",
        "\n",
        "    print(\"  \", right_action, \"\\t  \", state, \"\\t\", reward, \"\\t  \", is_Terminal)\n",
        "\n",
        "print(\"Total Number of steps to Reach Terminal:\", count)\n",
        "print(\"Final Reward:\", total_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jtOla8SPDc8"
      },
      "source": [
        "### The right policy most of the time reaches the goal state in just 3 steps which is expected"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7ef-dmmPDc8"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv1ymbPRPDc8"
      },
      "source": [
        "## Q1. Find an optimal policy to navigate the given environment using Policy Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqDXzKcpPDc8"
      },
      "source": [
        "###  Method 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWPfrUgMPDc8"
      },
      "source": [
        "#### This is the method according to the actual stochastic set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNbeIZqRPDc8",
        "outputId": "ca7a5985-d1a6-4858-fb57-2767bc4c5d00"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "env = StochasticMazeEnv()\n",
        "env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1ycUAUVXPDc8"
      },
      "outputs": [],
      "source": [
        "def reward(s,a, prob_dynamics):\n",
        "    s=int(s)\n",
        "    a = int(a)\n",
        "    x=0\n",
        "    for j in range(3):\n",
        "        x = x + (prob_dynamics[s][a][j][0])*(prob_dynamics[s][a][j][2])\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVp-yrucPDc8"
      },
      "source": [
        "* $R_s^a$ function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "T0741pWNPDc8"
      },
      "outputs": [],
      "source": [
        "def p_ss(policy, prob_dynamics):\n",
        "    P = np.zeros([12,12])\n",
        "    for i in range(12):\n",
        "            for j in range(12):\n",
        "                if j == prob_dynamics[i][policy[i]][0][1]:\n",
        "                    P[i][j] = 0.8\n",
        "                if j == prob_dynamics[i][policy[i]][1][1]:\n",
        "                    P[i][j] = P[i][j] + 0.1\n",
        "                if j == prob_dynamics[i][policy[i]][2][1]:\n",
        "                    P[i][j] = P[i][j] + 0.1\n",
        "                else:\n",
        "                    P[i][j] = P[i][j] + 0\n",
        "    return P"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFiv6NAsPDc8"
      },
      "source": [
        "this function, gives us $P_{ss'}^\\pi$ matrix 12*12\n",
        " - for i in range(12): represents for current state 'i'\n",
        " - for j in range(12): represents for the corresponding next state 'j' and checks if it can go to this state and gives the corresponding probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "R5142iDqPDc9"
      },
      "outputs": [],
      "source": [
        "def val_func1(policy, P_ss, gamma, prob_dynamics):\n",
        "    a = np.eye(12)-gamma*P_ss\n",
        "    A = np.matrix(a)\n",
        "    r = np.array(np.zeros(12))\n",
        "    for i in range(12):\n",
        "        r[i] = reward(i,policy[i], prob_dynamics)\n",
        "    R = np.transpose(r)\n",
        "    A_inv = np.linalg.inv(A)\n",
        "    V_New = np.dot(A_inv, R)\n",
        "    return V_New"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88Ig7qHePDc9"
      },
      "source": [
        "- function of $V_{\\pi}$ matrix, (12,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "q4okXLQpPDc9"
      },
      "outputs": [],
      "source": [
        "def policy_improvement1(policy,V, P_ss,gamma,prob_dynamics):\n",
        "    V_matrix = V.reshape(12,1)\n",
        "    q = np.zeros([12,4])\n",
        "    policy_New = np.zeros(12)\n",
        "    for i in range(12):\n",
        "        A = [0,0,0,0]   # to store values of action value function for every action\n",
        "        for j in range(4):\n",
        "            x = 0\n",
        "            y = 0\n",
        "            for k in range(3):\n",
        "                x = x + (prob_dynamics[i][j][k][0])*(prob_dynamics[i][j][k][2])\n",
        "            for k in range(3):\n",
        "                y = y + gamma*(float(V_matrix[prob_dynamics[i][j][k][1]][0]))*(prob_dynamics[i][j][k][0])\n",
        "            A[j] = x+y\n",
        "        policy_New[i] = np.argmax(A)\n",
        "    return policy_New"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rygvqRwZPDc9"
      },
      "source": [
        "Policy improvement function, gives $\\pi_k$\n",
        "- checks for the action for which gives max{$q_s^a$}\n",
        "- x is the probabilistic reward for intended and orthogonal actions\n",
        "- y is $\\Sigma P_{ss'}^{\\pi}*V_{\\pi}(s')$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOoIWvnyPDc9",
        "outputId": "edd54781-a6f3-4a0d-9a8e-aed796a46505"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
            "[2. 2. 2. 0. 1. 0. 0. 0. 0. 0. 3. 3.]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 0. 1. 3.]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-be3033f3824d>:13: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  y = y + gamma*(float(V_matrix[prob_dynamics[i][j][k][1]][0]))*(prob_dynamics[i][j][k][0])\n"
          ]
        }
      ],
      "source": [
        "gamma = 0.69\n",
        "policy = [3 for i in range(12)]\n",
        "\n",
        "# policy = []\n",
        "# for i in range(12):\n",
        "#     element = int(input(f\"Enter element {i + 1}: \"))\n",
        "#     array.append(element)\n",
        "\n",
        "count = 0\n",
        "prob_dynamics = env.P()\n",
        "P_ss = p_ss(policy, prob_dynamics)\n",
        "V_0 = val_func1(policy, P_ss, gamma, prob_dynamics)\n",
        "print(policy)\n",
        "policy_new = policy_improvement1(policy,V_0,P_ss,gamma,prob_dynamics)\n",
        "print(policy_new)\n",
        "count = 1\n",
        "while((policy_new!=policy).any()):\n",
        "    policy = policy_new\n",
        "    P_ss = p_ss(policy, prob_dynamics)\n",
        "    V_new = val_func1(policy, P_ss, gamma, prob_dynamics)\n",
        "    policy_new = policy_improvement1(policy,V_new, P_ss,gamma,prob_dynamics)\n",
        "    count += 1\n",
        "    print(policy_new)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pt1Zj290PDc9"
      },
      "source": [
        "#### - uncomment the commented lines to take the input of customized initial policies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rv5ktAepPDc9",
        "outputId": "24a6d591-9e37-4d63-9af3-208a24143613"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        }
      ],
      "source": [
        "print(count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSazuY8GPDc9"
      },
      "source": [
        "- counts the number of iterations our policy took"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ca-A2eJPDc9"
      },
      "source": [
        "#### Inferences\n",
        "- the actual optimistic policy will be [ 2, 2, 2, -, 1, -, 1, -, 1 (or) 2, 2, 1, 0]\n",
        "- '-' means any action doesn't matter as the episode terminates in these states\n",
        "- as we can see, the optimal policy we got is almost the actual optimal policy expect for the state 11\n",
        "- for different $\\gamma$, the converged policy changes\n",
        "- for our set-up, **$\\gamma $** from **0.69 to 0.9** seem to give us the actual optimal policy\n",
        "- it took __4 updates__ of policies for the policy to converge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1t7pgPiPDc9"
      },
      "source": [
        "### Method 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z884p5IwPDc9"
      },
      "source": [
        "#### This method considers deterministic setup for the state we are currently in and considers stochastic setup for other states\n",
        "- meaning we are taking intended action with probability 1 from the current state and taking the actual stochastic setup from the next state\n",
        "- the function used in Method 1 are changed accordingly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4VnV5GBrPDc-"
      },
      "outputs": [],
      "source": [
        "def val_func(policy, P_ss, gamma, prob_dynamics):\n",
        "    a = np.eye(12)-gamma*P_ss\n",
        "    A = np.matrix(a)\n",
        "    r = [(prob_dynamics[i][policy[i]][0][2]) for i in range(12)]\n",
        "    R = np.transpose(r)\n",
        "    A_inv = np.linalg.inv(A)\n",
        "    V_New = np.dot(A_inv, R)\n",
        "    return V_New"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "UbI0MIiBPDc-"
      },
      "outputs": [],
      "source": [
        "def policy_improvement(policy,V, prob_dynamics,gamma):\n",
        "    V = np.array(np.transpose(V))\n",
        "    q = np.zeros([12,4])\n",
        "    policy_New = np.zeros(12)\n",
        "    for i in range(12):\n",
        "        for j in range(4):\n",
        "            q[i][j]= prob_dynamics[i][j][0][2] + gamma*(V[prob_dynamics[i][j][0][1]])\n",
        "        if q[i][int(policy[i])] != np.max(q[i]):\n",
        "            policy_New[i] = np.argmax(q[i])\n",
        "        else:\n",
        "            policy_New[i] = policy[i]\n",
        "    return policy_New"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4lTGpSxPDc-",
        "outputId": "806dd8ae-7c8f-49ef-bf00-dac65fb191be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "[2. 2. 2. 2. 1. 2. 1. 2. 1. 0. 2. 2.]\n",
            "[2. 2. 2. 2. 1. 2. 1. 2. 1. 0. 1. 0.]\n",
            "[2. 2. 2. 2. 1. 2. 1. 2. 1. 2. 1. 0.]\n",
            "[2. 2. 2. 2. 1. 2. 1. 2. 1. 2. 1. 0.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-90d18128bd3c>:7: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  q[i][j]= prob_dynamics[i][j][0][2] + gamma*(V[prob_dynamics[i][j][0][1]])\n"
          ]
        }
      ],
      "source": [
        "# Let our initial policy be 'policy' givese the optimal action(indicated by 0,1,2,3) for every state(index)\n",
        "gamma = 0.8588\n",
        "\n",
        "policy = [2 for i in range(12)]\n",
        "count = 0;\n",
        "print(policy)\n",
        "prob_dynamics = env.P()\n",
        "P_ss = p_ss(policy, prob_dynamics)\n",
        "V = val_func(policy, P_ss, gamma, prob_dynamics)\n",
        "policy_new = policy_improvement(policy,V,prob_dynamics,gamma)\n",
        "count = 1\n",
        "print(policy_new)\n",
        "while((policy_new!=policy).any()):\n",
        "    policy = policy_new\n",
        "    P_ss = p_ss(policy, prob_dynamics)\n",
        "    V_new = val_func(policy, P_ss, gamma, prob_dynamics)\n",
        "    policy_new = policy_improvement(policy,V_new, prob_dynamics,gamma)\n",
        "    count += 1;\n",
        "    print(policy_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zt_YNNYePDc-",
        "outputId": "894f9d68-ac81-424b-aa7a-8135aec30c80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        }
      ],
      "source": [
        "print(count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_OrREn9PDc-"
      },
      "source": [
        "#### Inferences\n",
        "- this is simpler to code and converges to the actual optimal policy\n",
        "- for our set-up, **$\\gamma $** from **0.0016 to 0.8588** seem to give us the actual optimal policy\n",
        "- for $\\gamma > 0.8588$, our algorithm seem to oscillate for the states just before the terminal states\n",
        "- this algorithm also converges in __4__ updates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdXEL6V2PDc-"
      },
      "source": [
        "## Q2. Find an optimal policy to navigate the given environment using Value Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux4iVzOYPDc-"
      },
      "source": [
        "###  Method 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO7vJIwgPDdB"
      },
      "source": [
        "#### This is the method according to the actual stochastic set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77BoufuIPDdB",
        "outputId": "1f73097e-43fb-4be8-92a4-98f07c5494ca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "YTA4Imm9PDdC"
      },
      "outputs": [],
      "source": [
        "def P_ss_every_state(s,a,prob_dynamics):\n",
        "    P_ssa = np.zeros(12)\n",
        "    for j in range(12):\n",
        "        if j == prob_dynamics[s][a][0][1]:\n",
        "            P_ssa[j] = 0.8\n",
        "        if j == prob_dynamics[s][a][1][1]:\n",
        "            P_ssa[j] = P_ssa[j] + 0.1\n",
        "        if j == prob_dynamics[s][a][2][1]:\n",
        "            P_ssa[j] = P_ssa[j] + 0.1\n",
        "        else:\n",
        "            P_ssa[j] = P_ssa[j] + 0\n",
        "    return P_ssa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LoIGEmzPDdC"
      },
      "source": [
        "- gives $P_{ss'}^a$ value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ES_Rd2sNPDdC"
      },
      "outputs": [],
      "source": [
        "def p_ss(policy, prob_dynamics):\n",
        "    P = np.zeros([12,12])\n",
        "    for i in range(12):\n",
        "            for j in range(12):\n",
        "                if j == prob_dynamics[i][policy[i]][0][1]:\n",
        "                    P[i][j] = 0.8\n",
        "                if j == prob_dynamics[i][policy[i]][1][1]:\n",
        "                    P[i][j] = P[i][j] + 0.1\n",
        "                if j == prob_dynamics[i][policy[i]][2][1]:\n",
        "                    P[i][j] = P[i][j] + 0.1\n",
        "                else:\n",
        "                    P[i][j] = P[i][j] + 0\n",
        "    return P"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFEhNDuxPDdC"
      },
      "source": [
        "this function, gives us $P_{ss'}^\\pi$ matrix 12*12\n",
        " - for i in range(12): represents for current state 'i'\n",
        " - for j in range(12): represents for the corresponding next state 'j' and checks if it can go to this state and gives the corresponding probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ZzW4nY3TPDdC"
      },
      "outputs": [],
      "source": [
        "def reward(s,a, prob_dynamics):\n",
        "    s=int(s)\n",
        "    a = int(a)\n",
        "    x=0\n",
        "    for j in range(3):\n",
        "        x = x + (prob_dynamics[s][a][j][0])*(prob_dynamics[s][a][j][2])\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xOJhPdXPDdC"
      },
      "source": [
        "* $R_s^a$ function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "hDTBZDnkPDdC"
      },
      "outputs": [],
      "source": [
        "def val_func(policy, P_ss, gamma, prob_dynamics):\n",
        "    a = np.eye(12)-gamma*P_ss\n",
        "    A = np.matrix(a)\n",
        "    r = np.array(np.zeros(12))\n",
        "    for i in range(12):\n",
        "        r[i] = reward(i,policy[i], prob_dynamics)\n",
        "    R = np.transpose(r)\n",
        "    A_inv = np.linalg.inv(A)\n",
        "    V_New = np.dot(A_inv, R)\n",
        "    return V_New"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrvE-zXIPDdC"
      },
      "source": [
        "- function of $V_{\\pi}$ matrix, (12,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "seP9V_6cPDdC"
      },
      "outputs": [],
      "source": [
        "def val_iter_func(V, gamma, prob_dynamics,policy_P):\n",
        "    V_iter_new = np.matrix(np.zeros([12,1]))\n",
        "    V_matrix = V.reshape(12,1)\n",
        "    P_n = np.zeros(12)\n",
        "    for i in range(12):\n",
        "        A = [0,0,0,0]\n",
        "        for j in range(4):\n",
        "            x = 0\n",
        "            y = 0\n",
        "            for k in range(3):\n",
        "                x = x + (prob_dynamics[i][j][k][0])*(prob_dynamics[i][j][k][2])\n",
        "            for k in range(3):\n",
        "                y = y + gamma*(float(V_matrix[prob_dynamics[i][j][k][1]][0]))*(prob_dynamics[i][j][k][0])\n",
        "            A[j] = x+y\n",
        "        V_iter_new[i][0] = np.max(A)\n",
        "        P_n[i] = np.argmax(A)\n",
        "    return V_iter_new, P_n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh10tjTDPDdC"
      },
      "source": [
        "- this is value improvement function for value iteration algorithm\n",
        "- i.e., $V_{k+1} = max_a${$ R_s^a + \\gamma (\\Sigma P_{ss'}^a*V_k(s'))$}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bV0SeMz0PDdC",
        "outputId": "85184ee6-004a-4c09-83a0-4632445543cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.29665009  0.50970821  0.80062332  0.          0.00639977  0.\n",
            "  -0.76314726  0.         -0.1135658  -0.17154318 -0.24558417 -0.29459459]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 0. 0. 3.]\n",
            "[[ 0.29665009  0.50970821  0.80062332  0.          0.15702002  0.\n",
            "   0.28592875  0.         -0.02637376 -0.0976129  -0.17667538 -0.21278549]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 0. 1. 3.]\n",
            "[[ 0.30719351  0.50970821  0.87405865  0.          0.17810685  0.\n",
            "   0.35936407  0.          0.06925214 -0.03843511  0.12839222 -0.15642213]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[ 0.30940762  0.55083199  0.88433959  0.          0.18696332  0.\n",
            "   0.40562833  0.          0.09189703  0.05651873  0.17760387 -0.04804991]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[ 0.33321188  0.56234665  0.88829775  0.          0.18944313  0.\n",
            "   0.41462415  0.          0.10508856  0.09737079  0.21774468 -0.01290532]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.34149997 0.56617527 0.88920453 0.         0.20312069 0.\n",
            "  0.41747043 0.         0.11026031 0.12556893 0.22810211 0.01203365]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.3451816  0.56721908 0.88946725 0.         0.20967688 0.\n",
            "  0.41817747 0.         0.12025563 0.13531683 0.23341562 0.01957954]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.34648278 0.56751233 0.88953513 0.         0.21265646 0.\n",
            "  0.41837408 0.         0.12530913 0.13965711 0.23502213 0.02308332]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.34694665 0.5675914  0.88955364 0.         0.21380226 0.\n",
            "  0.41842586 0.         0.12763525 0.14116439 0.23568132 0.02422822]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.34710361 0.56761284 0.88955857 0.         0.21422244 0.\n",
            "  0.41843985 0.         0.12854524 0.14174455 0.23589596 0.02467751]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.34715601 0.56761859 0.88955989 0.         0.21436916 0.\n",
            "  0.41844359 0.         0.12888485 0.14194598 0.23597586 0.02482917]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.34717317 0.56762014 0.88956024 0.         0.21441905 0.\n",
            "  0.41844459 0.         0.12900489 0.14201892 0.23600267 0.02488452]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.34717873 0.56762056 0.88956034 0.         0.21443564 0.\n",
            "  0.41844486 0.         0.12904633 0.14204414 0.23601221 0.02490341]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.34718052 0.56762067 0.88956036 0.         0.21444108 0.\n",
            "  0.41844493 0.         0.12906029 0.14205302 0.23601545 0.02491008]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.34718109 0.5676207  0.88956037 0.         0.21444284 0.\n",
            "  0.41844495 0.         0.12906494 0.14205607 0.23601658 0.02491236]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.34718127 0.56762071 0.88956037 0.         0.21444341 0.\n",
            "  0.41844495 0.         0.12906646 0.14205713 0.23601696 0.02491315]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.34718132 0.56762071 0.88956037 0.         0.21444359 0.\n",
            "  0.41844496 0.         0.12906696 0.1420575  0.23601709 0.02491342]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.34718134 0.56762071 0.88956037 0.         0.21444364 0.\n",
            "  0.41844496 0.         0.12906712 0.14205762 0.23601714 0.02491351]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.34718135 0.56762071 0.88956037 0.         0.21444366 0.\n",
            "  0.41844496 0.         0.12906717 0.14205766 0.23601715 0.02491354]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.34718135 0.56762071 0.88956037 0.         0.21444367 0.\n",
            "  0.41844496 0.         0.12906719 0.14205768 0.23601716 0.02491355]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.34718135 0.56762071 0.88956037 0.         0.21444367 0.\n",
            "  0.41844496 0.         0.12906719 0.14205768 0.23601716 0.02491356]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.34718135 0.56762071 0.88956037 0.         0.21444367 0.\n",
            "  0.41844496 0.         0.1290672  0.14205769 0.23601716 0.02491356]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.34718135 0.56762071 0.88956037 0.         0.21444367 0.\n",
            "  0.41844496 0.         0.1290672  0.14205769 0.23601716 0.02491356]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.34718135 0.56762071 0.88956037 0.         0.21444367 0.\n",
            "  0.41844496 0.         0.1290672  0.14205769 0.23601716 0.02491356]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.34718135 0.56762071 0.88956037 0.         0.21444367 0.\n",
            "  0.41844496 0.         0.1290672  0.14205769 0.23601716 0.02491356]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.34718135 0.56762071 0.88956037 0.         0.21444367 0.\n",
            "  0.41844496 0.         0.1290672  0.14205769 0.23601716 0.02491356]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.34718135 0.56762071 0.88956037 0.         0.21444367 0.\n",
            "  0.41844496 0.         0.1290672  0.14205769 0.23601716 0.02491356]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.34718135 0.56762071 0.88956037 0.         0.21444367 0.\n",
            "  0.41844496 0.         0.1290672  0.14205769 0.23601716 0.02491356]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.34718135 0.56762071 0.88956037 0.         0.21444367 0.\n",
            "  0.41844496 0.         0.1290672  0.14205769 0.23601716 0.02491356]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.34718135 0.56762071 0.88956037 0.         0.21444367 0.\n",
            "  0.41844496 0.         0.1290672  0.14205769 0.23601716 0.02491356]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.34718135 0.56762071 0.88956037 0.         0.21444367 0.\n",
            "  0.41844496 0.         0.1290672  0.14205769 0.23601716 0.02491356]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.34718135 0.56762071 0.88956037 0.         0.21444367 0.\n",
            "  0.41844496 0.         0.1290672  0.14205769 0.23601716 0.02491356]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.34718135 0.56762071 0.88956037 0.         0.21444367 0.\n",
            "  0.41844496 0.         0.1290672  0.14205769 0.23601716 0.02491356]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.34718135 0.56762071 0.88956037 0.         0.21444367 0.\n",
            "  0.41844496 0.         0.1290672  0.14205769 0.23601716 0.02491356]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.34718135 0.56762071 0.88956037 0.         0.21444367 0.\n",
            "  0.41844496 0.         0.1290672  0.14205769 0.23601716 0.02491356]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.34718135 0.56762071 0.88956037 0.         0.21444367 0.\n",
            "  0.41844496 0.         0.1290672  0.14205769 0.23601716 0.02491356]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.34718135 0.56762071 0.88956037 0.         0.21444367 0.\n",
            "  0.41844496 0.         0.1290672  0.14205769 0.23601716 0.02491356]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.34718135 0.56762071 0.88956037 0.         0.21444367 0.\n",
            "  0.41844496 0.         0.1290672  0.14205769 0.23601716 0.02491356]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.34718135 0.56762071 0.88956037 0.         0.21444367 0.\n",
            "  0.41844496 0.         0.1290672  0.14205769 0.23601716 0.02491356]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-feb794149372>:13: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  y = y + gamma*(float(V_matrix[prob_dynamics[i][j][k][1]][0]))*(prob_dynamics[i][j][k][0])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.34718135 0.56762071 0.88956037 0.         0.21444367 0.\n",
            "  0.41844496 0.         0.1290672  0.14205769 0.23601716 0.02491356]]\n"
          ]
        }
      ],
      "source": [
        "policy = [2 for i in range(12)]\n",
        "gamma = 0.7\n",
        "P_ss2 = p_ss(policy, prob_dynamics)\n",
        "V_0 = val_func(policy, P_ss2, gamma, prob_dynamics)\n",
        "\n",
        "print(V_0)\n",
        "\n",
        "V_new = val_iter_func(V_0, gamma, prob_dynamics, policy)[0]\n",
        "\n",
        "Policy_new = val_iter_func(V_0, gamma, prob_dynamics,policy)[1]\n",
        "\n",
        "print(Policy_new)\n",
        "print(np.transpose(V_new))\n",
        "count = 1\n",
        "while((V_new != V_0).any()):\n",
        "    V_0 = V_new\n",
        "    V_new = val_iter_func(V_0, gamma, prob_dynamics, policy)[0]\n",
        "    Policy_new = val_iter_func(V_0, gamma, prob_dynamics,policy)[1]\n",
        "    print(Policy_new)\n",
        "    print(np.transpose(V_new))\n",
        "    count+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuzBPoiFPDdD",
        "outputId": "70695063-4472-4267-f68e-bace400c1362"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.34718135 0.56762071 0.88956037 0.         0.21444367 0.\n",
            "  0.41844496 0.         0.1290672  0.14205769 0.23601716 0.02491356]]\n"
          ]
        }
      ],
      "source": [
        "print(np.transpose(V_new))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nh2D40c4PDdD"
      },
      "source": [
        "- this is the converged value function( $V_\\pi$) matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMfKae8BPDdD",
        "outputId": "bee79974-0ddb-4559-d1de-6b9a236b7a99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n"
          ]
        }
      ],
      "source": [
        "print(Policy_new)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AwrSDK8PDdD"
      },
      "source": [
        "- this is the converged policy( $ \\pi$)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tbvOK8lPDdD",
        "outputId": "7098ff94-1f2b-49df-cc70-9fff27fa4bae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39\n"
          ]
        }
      ],
      "source": [
        "print(count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkvD_rD1PDdD"
      },
      "source": [
        "- number of updates this algorithm took"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dM_5Xv9kPDdD"
      },
      "source": [
        "#### Inferences\n",
        "- the actual optimistic policy will be [ 2, 2, 2, -, 1, -, 1, -, 1 (or) 2, 2, 1, 0]\n",
        "- '-' means any action doesn't matter as the episode terminates in these states\n",
        "- as we can see, the optimal policy we got is almost the actual optimal policy expect for the state 11\n",
        "- for different $\\gamma$, the converged policy changes\n",
        "- for our set-up, **$\\gamma $** from **0.69 to 0.9** seem to give us the actual optimal policy\n",
        "- it took __39 updates__ of policies for the policy to converge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8iE9Mj9PDdE"
      },
      "source": [
        "### Method 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjJ-jQu1PDdE"
      },
      "source": [
        "#### This method considers deterministic setup for the state we are currently in and considers stochastic setup for other states\n",
        "- meaning we are taking intended action with probability 1 from the current state and taking the actual stochastic setup from the next state\n",
        "- the function used in Method 1 are changed accordingly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1kq21NjPDdE",
        "outputId": "987db3ab-3dc9-4b49-c742-044249adf833"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "JXZkdv_TPDdE"
      },
      "outputs": [],
      "source": [
        "def P_ss_every_state(s,a,prob_dynamics):\n",
        "    P_ssa = np.zeros(12)\n",
        "    for j in range(12):\n",
        "        if j == prob_dynamics[s][a][0][1]:\n",
        "            P_ssa[j] = 0.8\n",
        "        if j == prob_dynamics[s][a][1][1]:\n",
        "            P_ssa[j] = P_ssa[j] + 0.1\n",
        "        if j == prob_dynamics[s][a][2][1]:\n",
        "            P_ssa[j] = P_ssa[j] + 0.1\n",
        "        else:\n",
        "            P_ssa[j] = P_ssa[j] + 0\n",
        "    return P_ssa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buYBxVb3PDdE"
      },
      "source": [
        "- gives $P_{ss'}^a$ value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "rbedbEKHPDdE"
      },
      "outputs": [],
      "source": [
        "def p_ss(policy, prob_dynamics):\n",
        "    P = np.zeros([12,12])\n",
        "    for i in range(12):\n",
        "            for j in range(12):\n",
        "                if j == prob_dynamics[i][policy[i]][0][1]:\n",
        "                    P[i][j] = 0.8\n",
        "                if j == prob_dynamics[i][policy[i]][1][1]:\n",
        "                    P[i][j] = P[i][j] + 0.1\n",
        "                if j == prob_dynamics[i][policy[i]][2][1]:\n",
        "                    P[i][j] = P[i][j] + 0.1\n",
        "                else:\n",
        "                    P[i][j] = P[i][j] + 0\n",
        "    return P"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKoul_1YPDdE"
      },
      "source": [
        "this function, gives us $P_{ss'}^\\pi$ matrix 12*12\n",
        " - for i in range(12): represents for current state 'i'\n",
        " - for j in range(12): represents for the corresponding next state 'j' and checks if it can go to this state and gives the corresponding probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Ra36anetPDdE"
      },
      "outputs": [],
      "source": [
        "def val_func(policy, P_ss, gamma, prob_dynamics):\n",
        "    a = np.eye(12)-gamma*P_ss\n",
        "    A = np.matrix(a)\n",
        "    r = [prob_dynamics[i][policy[i]][0][2] for i in range(12)]\n",
        "    R = np.transpose(np.matrix(r))\n",
        "    A_inv = np.linalg.inv(A)\n",
        "    V_New = np.dot(A_inv, R)\n",
        "    return V_New"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxXB8-gfPDdE"
      },
      "source": [
        "- function of $V_{\\pi}$ matrix, (12,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "uHyDUw8KPDdE"
      },
      "outputs": [],
      "source": [
        "def val_iter_func(V, gamma, prob_dynamics,policy_P):\n",
        "    V_iter_new = np.matrix(np.zeros([12,1]))\n",
        "    P_n = np.zeros(12)\n",
        "    for i in range(12):\n",
        "        A = [0,0,0,0]\n",
        "        for j in range(4):\n",
        "            V_s_next = np.matrix(np.zeros([12,1]))\n",
        "            for m in range(12):\n",
        "                #print(m)\n",
        "                V_s_next[m][0] = V[prob_dynamics[m][j][0][1]][0]\n",
        "            A[j] = prob_dynamics[i][j][0][2] + gamma*(np.dot(P_ss_every_state(i,j,prob_dynamics),V))\n",
        "        V_iter_new[i][0] = np.max(A)\n",
        "        P_n[i] = np.argmax(A)\n",
        "    return V_iter_new.reshape(12,1), np.array(P_n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLlEWzAoPDdE"
      },
      "source": [
        "- this is value improvement function for value iteration algorithm\n",
        "- i.e., $V_{k+1} = max_a${$ R_s^a + \\gamma (\\Sigma P_{ss'}^a*V_k(s'))$}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOk8h5JfPDdE",
        "outputId": "11da4680-f003-47d3-e903-efa30b225117"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "[[ 0.6029987   0.81291157  1.00693618  0.          0.1054581   0.\n",
            "  -0.92533405  0.         -0.08833291 -0.1173361  -0.12851318 -0.04255319]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 0. 3. 2.]\n",
            "[[ 0.6029987   0.81291157  1.00693618  0.          0.41796699  0.\n",
            "   0.59606321  0.          0.04422964 -0.09001352 -0.11097956 -0.04255319]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 0. 1. 2.]\n",
            "[[ 0.62956195  0.81291157  1.13625495  0.          0.4710935   0.\n",
            "   0.72538198  0.          0.27032592  0.00477386  0.38405481 -0.04255319]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.63633558 0.90084833 1.15823914 0.         0.49818802 0.\n",
            "  0.82431083 0.         0.33372706 0.25196883 0.4800485  0.24754025]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.69901137 0.93074683 1.16851675 0.         0.50740016 0.\n",
            "  0.84766904 0.         0.37855201 0.35926768 0.59298964 0.3374739 ]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.72545282 0.94281835 1.17137579 0.         0.55158576 0.\n",
            "  0.85664326 0.         0.39774678 0.45430846 0.62563798 0.42191824]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.73966476 0.94681466 1.17238162 0.         0.5770775  0.\n",
            "  0.85935022 0.         0.43750301 0.49266626 0.64699668 0.45129688]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.74575706 0.94817799 1.17269721 0.         0.59107521 0.\n",
            "  0.86026427 0.         0.46147709 0.51371101 0.65459501 0.46831798]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.74839178 0.94862436 1.17280173 0.         0.59759759 0.\n",
            "  0.86055656 0.         0.47482213 0.52245548 0.65845217 0.47493164]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.74947366 0.94877131 1.17283545 0.         0.600498   0.\n",
            "  0.86065248 0.         0.48113496 0.52656491 0.65995637 0.47811666]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.74991208 0.94881923 1.17284647 0.         0.60172675 0.\n",
            "  0.86068357 0.         0.48399313 0.52828636 0.66064162 0.47941025]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.75008638 0.94883487 1.17285005 0.         0.60223376 0.\n",
            "  0.86069371 0.         0.48521795 0.52904498 0.66091904 0.47998617]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.75015493 0.94883996 1.17285122 0.         0.60243848 0.\n",
            "  0.860697   0.         0.48573131 0.52936259 0.66103937 0.48022377]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.75018162 0.94884162 1.1728516  0.         0.60251989 0.\n",
            "  0.86069807 0.         0.48594115 0.52949841 0.6610888  0.48032579]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.75019193 0.94884216 1.17285172 0.         0.60255188 0.\n",
            "  0.86069842 0.         0.48602589 0.52955512 0.66110975 0.48036808]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.75019589 0.94884234 1.17285176 0.         0.60256433 0.\n",
            "  0.86069854 0.         0.48605966 0.529579   0.6611184  0.48038592]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.75019741 0.9488424  1.17285178 0.         0.60256915 0.\n",
            "  0.86069857 0.         0.48607303 0.52958894 0.66112202 0.48039331]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.75019799 0.94884241 1.17285178 0.         0.60257099 0.\n",
            "  0.86069859 0.         0.48607829 0.5295931  0.66112352 0.48039641]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.75019821 0.94884242 1.17285178 0.         0.6025717  0.\n",
            "  0.86069859 0.         0.48608034 0.52959482 0.66112415 0.48039769]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.75019829 0.94884242 1.17285178 0.         0.60257197 0.\n",
            "  0.86069859 0.         0.48608114 0.52959554 0.6611244  0.48039822]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.75019832 0.94884242 1.17285178 0.         0.60257207 0.\n",
            "  0.86069859 0.         0.48608146 0.52959584 0.66112451 0.48039844]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.75019833 0.94884242 1.17285178 0.         0.60257211 0.\n",
            "  0.86069859 0.         0.48608158 0.52959596 0.66112456 0.48039854]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.75019834 0.94884242 1.17285178 0.         0.60257212 0.\n",
            "  0.86069859 0.         0.48608162 0.52959601 0.66112457 0.48039857]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.75019834 0.94884242 1.17285178 0.         0.60257213 0.\n",
            "  0.86069859 0.         0.48608164 0.52959603 0.66112458 0.48039859]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.75019834 0.94884242 1.17285178 0.         0.60257213 0.\n",
            "  0.86069859 0.         0.48608165 0.52959604 0.66112459 0.4803986 ]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.75019834 0.94884242 1.17285178 0.         0.60257213 0.\n",
            "  0.86069859 0.         0.48608165 0.52959605 0.66112459 0.4803986 ]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.75019834 0.94884242 1.17285178 0.         0.60257213 0.\n",
            "  0.86069859 0.         0.48608165 0.52959605 0.66112459 0.4803986 ]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.75019834 0.94884242 1.17285178 0.         0.60257213 0.\n",
            "  0.86069859 0.         0.48608165 0.52959605 0.66112459 0.4803986 ]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.75019834 0.94884242 1.17285178 0.         0.60257213 0.\n",
            "  0.86069859 0.         0.48608165 0.52959605 0.66112459 0.4803986 ]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.75019834 0.94884242 1.17285178 0.         0.60257213 0.\n",
            "  0.86069859 0.         0.48608165 0.52959605 0.66112459 0.4803986 ]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.75019834 0.94884242 1.17285178 0.         0.60257213 0.\n",
            "  0.86069859 0.         0.48608165 0.52959605 0.66112459 0.4803986 ]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.75019834 0.94884242 1.17285178 0.         0.60257213 0.\n",
            "  0.86069859 0.         0.48608165 0.52959605 0.66112459 0.4803986 ]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.75019834 0.94884242 1.17285178 0.         0.60257213 0.\n",
            "  0.86069859 0.         0.48608165 0.52959605 0.66112459 0.4803986 ]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.75019834 0.94884242 1.17285178 0.         0.60257213 0.\n",
            "  0.86069859 0.         0.48608165 0.52959605 0.66112459 0.4803986 ]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.75019834 0.94884242 1.17285178 0.         0.60257213 0.\n",
            "  0.86069859 0.         0.48608165 0.52959605 0.66112459 0.4803986 ]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.75019834 0.94884242 1.17285178 0.         0.60257213 0.\n",
            "  0.86069859 0.         0.48608165 0.52959605 0.66112459 0.4803986 ]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.75019834 0.94884242 1.17285178 0.         0.60257213 0.\n",
            "  0.86069859 0.         0.48608165 0.52959605 0.66112459 0.4803986 ]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.75019834 0.94884242 1.17285178 0.         0.60257213 0.\n",
            "  0.86069859 0.         0.48608165 0.52959605 0.66112459 0.4803986 ]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.75019834 0.94884242 1.17285178 0.         0.60257213 0.\n",
            "  0.86069859 0.         0.48608165 0.52959605 0.66112459 0.4803986 ]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.75019834 0.94884242 1.17285178 0.         0.60257213 0.\n",
            "  0.86069859 0.         0.48608165 0.52959605 0.66112459 0.4803986 ]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.75019834 0.94884242 1.17285178 0.         0.60257213 0.\n",
            "  0.86069859 0.         0.48608165 0.52959605 0.66112459 0.4803986 ]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.75019834 0.94884242 1.17285178 0.         0.60257213 0.\n",
            "  0.86069859 0.         0.48608165 0.52959605 0.66112459 0.4803986 ]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.75019834 0.94884242 1.17285178 0.         0.60257213 0.\n",
            "  0.86069859 0.         0.48608165 0.52959605 0.66112459 0.4803986 ]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.75019834 0.94884242 1.17285178 0.         0.60257213 0.\n",
            "  0.86069859 0.         0.48608165 0.52959605 0.66112459 0.4803986 ]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.75019834 0.94884242 1.17285178 0.         0.60257213 0.\n",
            "  0.86069859 0.         0.48608165 0.52959605 0.66112459 0.4803986 ]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.75019834 0.94884242 1.17285178 0.         0.60257213 0.\n",
            "  0.86069859 0.         0.48608165 0.52959605 0.66112459 0.4803986 ]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.75019834 0.94884242 1.17285178 0.         0.60257213 0.\n",
            "  0.86069859 0.         0.48608165 0.52959605 0.66112459 0.4803986 ]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.75019834 0.94884242 1.17285178 0.         0.60257213 0.\n",
            "  0.86069859 0.         0.48608165 0.52959605 0.66112459 0.4803986 ]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.75019834 0.94884242 1.17285178 0.         0.60257213 0.\n",
            "  0.86069859 0.         0.48608165 0.52959605 0.66112459 0.4803986 ]]\n",
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
            "[[0.75019834 0.94884242 1.17285178 0.         0.60257213 0.\n",
            "  0.86069859 0.         0.48608165 0.52959605 0.66112459 0.4803986 ]]\n"
          ]
        }
      ],
      "source": [
        "policy = [2 for i in range(12)]\n",
        "gamma = 0.85\n",
        "prob_dynamics= env.P()\n",
        "P_ss2 = p_ss(policy, prob_dynamics)\n",
        "V_0 = val_func(policy, P_ss2, gamma, prob_dynamics)\n",
        "print(policy)\n",
        "print(np.transpose(V_0))\n",
        "V_new = val_iter_func(V_0, gamma, prob_dynamics, policy)[0]\n",
        "Policy_new = val_iter_func(V_0, gamma, prob_dynamics,policy)[1]\n",
        "print(Policy_new)\n",
        "print(np.transpose(V_new))\n",
        "count = 1\n",
        "while((V_new != V_0).any()):\n",
        "    V_0 = V_new\n",
        "    V_new = val_iter_func(V_0, gamma, prob_dynamics, policy)[0]\n",
        "    Policy_new = val_iter_func(V_0, gamma, prob_dynamics,policy)[1]\n",
        "    print(Policy_new)\n",
        "    print(np.transpose(V_new))\n",
        "    count+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9u4LzblhPDdF",
        "outputId": "67579f85-0dc3-4a85-f796-7618ec250f70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.75019834 0.94884242 1.17285178 0.         0.60257213 0.\n",
            "  0.86069859 0.         0.48608165 0.52959605 0.66112459 0.4803986 ]]\n"
          ]
        }
      ],
      "source": [
        "print(np.transpose(V_new))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbENC_LwPDdF"
      },
      "source": [
        "- this is the converged value function( $V_\\pi$) matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_u1AmBuwPDdF",
        "outputId": "e4b06f10-bbf3-4eb8-f909-2109a156f784"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2. 2. 2. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n"
          ]
        }
      ],
      "source": [
        "print(Policy_new)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hkqL1fdPDdF"
      },
      "source": [
        "- this is the converged policy( $ \\pi$)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBO5YXhJPDdF",
        "outputId": "a52958cb-2072-424a-ae43-6d05636ce6ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "49\n"
          ]
        }
      ],
      "source": [
        "print(count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6axznszYPDdF"
      },
      "source": [
        "- the updates it took for our algorithm to converge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IowcYFTLPDdF"
      },
      "source": [
        "#### Inferences\n",
        "- this is simpler to code and converges to the actual optimal policy\n",
        "- for our set-up, **$\\gamma $** from **0.0016 to 0.8588** seem to give us the actual optimal policy\n",
        "- for $\\gamma > 0.8588$, our algorithm seem to oscillate for the states just before the terminal states\n",
        "- this algorithm also converges in __48__ updates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fehO4ABPDdF"
      },
      "source": [
        "## Q3. Compare PI and VI in terms of convergence. Is the policy obtained by both same?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "of6Ez16-PDdF"
      },
      "source": [
        "### Answer:\n",
        "- #### Both PI and VI converge to the same policy, but PI converges in fewer iterations than VI\n",
        "- #### In VI, the policy convergence took the same number of iterations as PI\n",
        "- #### But the convergence of value function in VI took far more iterations\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "4cafede8658e71bdc4b7180bcd658951c639327337cbd78715b7c29dc66075fe"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}